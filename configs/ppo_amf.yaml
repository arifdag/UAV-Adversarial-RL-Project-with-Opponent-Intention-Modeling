algorithm: IntentPPO
policy: AMFPolicy
total_timesteps: 3000000
n_envs: 16  # Increased parallel environments for more stable gradients
amf_lambda: 0.4            # Increased final λ after warm-up (aux loss weight)
lambda_schedule: linear    # Ramp λ from 0→amf_lambda
lambda_warmup_steps: 500000
freeze_feature_steps: 200000
balanced_loss: true
log_h_opp: true
checkpoint_freq: 100_000
eval_freq: 20_000
gamma: 0.99
learning_rate: 3e-4        # Increased initial learning rate
learning_rate_schedule: linear  # Linear decay over training steps
n_steps: 4096
batch_size: 512            # Increased batch size for more stable updates
gae_lambda: 0.95
ent_coef: 0.05             # Increased entropy coefficient for more exploration
clip_range: 0.3            # Start with larger clip range
clip_range_schedule: linear  # Anneal from 0.3 to 0.1
n_epochs: 6
vf_coef: 1.0               # Increased value loss coefficient for better value fit
max_grad_norm: 0.5
# Note: If scheduling for aux loss weight is not implemented, consider adding it in the code. 